{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, load_diabetes, fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from spacetree import QuantileNormDecisionTree\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_tree(tree, ref_points, feature_names=None, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively interprets the decision tree and prints it in a human-readable format.\n",
    "    \n",
    "    Parameters:\n",
    "        tree (dict): The decision tree structure to interpret.\n",
    "        ref_points (ndarray): Array of reference points used by the tree.\n",
    "        feature_names (list or None): Optional custom feature names for interpretation.\n",
    "        depth (int): Current depth of the tree for indentation.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth  # Indentation for better visualization\n",
    "    \n",
    "    if tree['is_leaf']:\n",
    "        # Print details for a leaf node\n",
    "        print(f\"{indent}Leaf: Predict value = {tree['value']:.4f}, \"\n",
    "              f\"Samples = {tree['n_samples']}, Impurity = {tree['impurity']:.4f}\")\n",
    "        return\n",
    "    \n",
    "    # Generate feature description\n",
    "    if feature_names is None:\n",
    "        feature_description = f\"distance from reference point {tree['ref_idx']}\"\n",
    "    else:\n",
    "        feature_description = feature_names[tree['ref_idx']]\n",
    "    \n",
    "    # Print the current node's condition\n",
    "    print(f\"{indent}Node: If {feature_description} <= {tree['threshold']:.4f}:\")\n",
    "    \n",
    "    # Print the left subtree\n",
    "    print(f\"{indent}  Left subtree:\")\n",
    "    interpret_tree(tree['left'], ref_points, feature_names, depth + 2)\n",
    "    \n",
    "    # Print the right subtree\n",
    "    print(f\"{indent}  Else (distance > {tree['threshold']:.4f}):\")\n",
    "    print(f\"{indent}  Right subtree:\")\n",
    "    interpret_tree(tree['right'], ref_points, feature_names, depth + 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "# Features and target\n",
    "X = diamonds.drop('price', axis=1)\n",
    "y = diamonds['price']\n",
    "\n",
    "# Categorical columns to encode\n",
    "categorical_columns = ['cut', 'color', 'clarity']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "encoded_categories = encoder.fit_transform(X[categorical_columns]).toarray()  # Convert to dense array\n",
    "\n",
    "# Create a DataFrame with encoded categorical features\n",
    "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "numerical_features = X.drop(categorical_columns, axis=1).reset_index(drop=True)\n",
    "X = pd.concat([numerical_features, encoded_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3464.74\n",
      "MSE: 2554.94\n"
     ]
    }
   ],
   "source": [
    "tree = QuantileNormDecisionTree(max_depth=3, n_quantiles=500, min_samples_split=2, n_ref_points=200, impurity_method=\"se\")\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Tahmin yap\n",
    "predictions = tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "predictions = tree.predict(X_train)\n",
    "mse = mean_squared_error(y_train, predictions)\n",
    "print(f\"MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: If distance from reference point 75 <= 6.5787:\n",
      "  Left subtree:\n",
      "    Node: If distance from reference point 170 <= 5.7590:\n",
      "      Left subtree:\n",
      "        Node: If distance from reference point 75 <= 5.9359:\n",
      "          Left subtree:\n",
      "            Leaf: Predict value = 204.1765, Samples = 34, Impurity = 81088.9412\n",
      "          Else (distance > 5.9359):\n",
      "          Right subtree:\n",
      "            Leaf: Predict value = 157.9070, Samples = 43, Impurity = 120089.6279\n",
      "      Else (distance > 5.7590):\n",
      "      Right subtree:\n",
      "        Node: If distance from reference point 2 <= 4.3650:\n",
      "          Left subtree:\n",
      "            Leaf: Predict value = 211.0345, Samples = 29, Impurity = 87196.9655\n",
      "          Else (distance > 4.3650):\n",
      "          Right subtree:\n",
      "            Leaf: Predict value = 271.8276, Samples = 29, Impurity = 32202.1379\n",
      "  Else (distance > 6.5787):\n",
      "  Right subtree:\n",
      "    Node: If distance from reference point 104 <= 5.8267:\n",
      "      Left subtree:\n",
      "        Node: If distance from reference point 80 <= 8.1158:\n",
      "          Left subtree:\n",
      "            Leaf: Predict value = 120.0612, Samples = 49, Impurity = 177552.8163\n",
      "          Else (distance > 8.1158):\n",
      "          Right subtree:\n",
      "            Leaf: Predict value = 187.4667, Samples = 30, Impurity = 128545.4667\n",
      "      Else (distance > 5.8267):\n",
      "      Right subtree:\n",
      "        Node: If distance from reference point 73 <= 4.5822:\n",
      "          Left subtree:\n",
      "            Leaf: Predict value = 81.5429, Samples = 70, Impurity = 88389.3714\n",
      "          Else (distance > 4.5822):\n",
      "          Right subtree:\n",
      "            Leaf: Predict value = 118.7021, Samples = 47, Impurity = 130619.8298\n"
     ]
    }
   ],
   "source": [
    "interpret_tree(tree.tree, tree.ref_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4440.95\n",
      "MSE: 3233.95\n"
     ]
    }
   ],
   "source": [
    "# Modeli oluştur ve eğit\n",
    "tree = DecisionTreeRegressor(max_depth=2)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Tahmin yap\n",
    "predictions = tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "predictions = tree.predict(X_train)\n",
    "mse = mean_squared_error(y_train, predictions)\n",
    "print(f\"MSE: {mse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
